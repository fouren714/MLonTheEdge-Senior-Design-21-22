{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SeniorDesignYolov5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQhuRVIbxaDq"
      },
      "source": [
        "#FIX ROT 90 LABELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW1cE9kAVttk",
        "outputId": "db6411b2-ea83-40f3-a138-775d7aa50e68"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "from os import path\n",
        "import shutil\n",
        "import csv\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 9893, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 9893 (delta 51), reused 32 (delta 11), pack-reused 9801\u001b[K\n",
            "Receiving objects: 100% (9893/9893), 10.36 MiB | 21.57 MiB/s, done.\n",
            "Resolving deltas: 100% (6837/6837), done.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (2.7.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.10.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (3.10.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.41.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.6.0)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 thop-0.0.31.post2005241907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUu8iBTr05R_"
      },
      "source": [
        "# The next few cells can be used to download and train on LPCV Data Directly\n",
        "# You must upload all LPCV M4V and CSV files to create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPJhOAlzBj69",
        "outputId": "f54833b1-5746-4b96-9e44-94c2a0c195fe"
      },
      "source": [
        "%mkdir LPCV\n",
        "%cd LPCV\n",
        "%mkdir images\n",
        "%mkdir labels\n",
        "%mkdir valid\n",
        "%mkdir valid/images\n",
        "%mkdir valid/labels\n",
        "%mkdir train\n",
        "%mkdir train/images\n",
        "%mkdir train/labels\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5/LPCV\n",
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSrdeTH-eL96"
      },
      "source": [
        "%rm -r LPCV/train\n",
        "%rm -r LPCV/valid\n",
        "%mkdir ./LPCV/valid\n",
        "%mkdir ./LPCV/valid/images\n",
        "%mkdir ./LPCV/valid/labels\n",
        "%mkdir ./LPCV/train\n",
        "%mkdir ./LPCV/train/images\n",
        "%mkdir ./LPCV/train/labels"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDV2lp94A7bI"
      },
      "source": [
        "def capture_frames(video, variety):\n",
        "  vidcap = cv2.VideoCapture(video)\n",
        "  success,image = vidcap.read()\n",
        "  count = 0\n",
        "  while success:\n",
        "    cv2.imwrite(\"LPCV/images/{}_frame{}.jpg\".format(variety,count), image)     # save frame as JPEG file      \n",
        "    success,image = vidcap.read()\n",
        "    count += 1\n",
        "  print(\"Finished Reading \" + video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCl60JNDDwCY"
      },
      "source": [
        "def create_labels(video_csv, variety):\n",
        "  with open(video_csv, newline='') as csvfile:\n",
        "    csv_reader = csv.reader(csvfile)\n",
        "    yolo_labels = []\n",
        "    for row in csv_reader:\n",
        "      frame = row[0]\n",
        "      if row[0] == \"Frame\":\n",
        "        continue\n",
        "      else:\n",
        "        with open(\"LPCV/labels/\"+variety+\"_frame\" + str(frame) + \".txt\", \"a\") as yololabelfile:\n",
        "          yoloarray = row\n",
        "          del yoloarray[2:3]\n",
        "          del yoloarray[0:1]\n",
        "          for item in yoloarray:\n",
        "            yololabelfile.write(\"%s \" % item)\n",
        "          yololabelfile.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN-3odZ3B08j",
        "outputId": "bc400974-1d61-4a03-95ae-b34f2dac2a78"
      },
      "source": [
        "capture_frames('4p1b_01A2.m4v','4p1b')\n",
        "capture_frames('5p2b_01A1.m4v','5p2b')\n",
        "capture_frames('5p4b_01A2.m4v','5p4b')\n",
        "capture_frames('5p5b_03A1.m4v','5p5b')\n",
        "capture_frames('7p3b_02M.m4v','7p3b')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Reading 4p1b_01A2.m4v\n",
            "Finished Reading 5p2b_01A1.m4v\n",
            "Finished Reading 5p4b_01A2.m4v\n",
            "Finished Reading 5p5b_03A1.m4v\n",
            "Finished Reading 7p3b_02M.m4v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhQMrqupC-Ed"
      },
      "source": [
        "create_labels('4p1b_01A2.csv','4p1b')\n",
        "create_labels('5p2b_01A1.csv','5p2b')\n",
        "create_labels('5p4b_01A2.csv','5p4b')\n",
        "create_labels('5p5b_03A1.csv','5p5b')\n",
        "create_labels('7p3b_02M.csv','7p3b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4bqrTqv1EV"
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "from shutil import copyfile\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fWTjgO-uKhw"
      },
      "source": [
        "for root, dirs, files in os.walk('LPCV/labels/', topdown=False):\n",
        "    for filename in files:\n",
        "      num = filename[10:filename.find(\".\")]\n",
        "      name = filename[:10]\n",
        "      video_file = name+num+\".jpg\"\n",
        "      copyfile('LPCV/labels/'+filename, 'LPCV/train/labels/'+filename)\n",
        "      copyfile('LPCV/images/'+video_file, 'LPCV/train/images/'+video_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNuC86IpB6GT"
      },
      "source": [
        "!zip -r LPCVtrain.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m952YYfPVvKo"
      },
      "source": [
        "#To save LPCV image and labels, zip and download the LPCV folder at this point. In the future the folder can be uploaded and the above cells are uneccessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88bH6IlXcusr"
      },
      "source": [
        "%rm -r ./LPCV"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS7RMLg7YGFY",
        "outputId": "44557db2-3555-48b9-a713-edcccd68ef35"
      },
      "source": [
        "!unzip LPCVtrain.zip"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  LPCVtrain.zip\n",
            "   creating: LPCV/train/\n",
            "   creating: LPCV/train/labels/\n",
            "  inflating: LPCV/train/labels/5p4b_frame929.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame7.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame8.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame896.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3678.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame932.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame7.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame0.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame1.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame898.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame2.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame2.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame4.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame897.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame2.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame1.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame935.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame2.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3674.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame5.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame928.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame927.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame10.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame930.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame9.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame0.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame0.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame934.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame6.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame901.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame928.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame10.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame6.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame8.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame3.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame2.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame3.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame8.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame905.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame909.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3673.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame931.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame6.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame915.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame927.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame900.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3677.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame913.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame934.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3672.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame3.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame1.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame932.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3675.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame1.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame4.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame911.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame903.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame8.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame904.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame3.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame5.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame0.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame910.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame933.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3671.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame899.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame895.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame0.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame929.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame8.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame9.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame10.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame912.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame5.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame7.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3669.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame916.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame4.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame902.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3679.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame1.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame925.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame5.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame7.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame9.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame930.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame4.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame7.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame924.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame9.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame931.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame10.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3670.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame5.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame4.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame926.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame907.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame914.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame925.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame9.txt  \n",
            "  inflating: LPCV/train/labels/5p4b_frame933.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame908.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame6.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame917.txt  \n",
            "  inflating: LPCV/train/labels/4p1b_frame10.txt  \n",
            "  inflating: LPCV/train/labels/5p5b_frame926.txt  \n",
            "  inflating: LPCV/train/labels/7p3b_frame3676.txt  \n",
            "  inflating: LPCV/train/labels/5p2b_frame6.txt  \n",
            "   creating: LPCV/train/images/\n",
            "  inflating: LPCV/train/images/4p1b_frame917.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame916.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame4.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame913.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame900.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame5.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame6.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame7.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame2.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame9.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame902.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame8.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame912.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3678.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame3.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame10.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame897.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame928.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame925.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame896.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame1.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame914.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame4.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame4.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame908.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame1.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3669.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame5.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame899.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame895.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame7.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame928.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame10.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3671.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame930.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame910.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3670.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame1.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3679.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3677.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame6.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame0.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame929.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame8.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame930.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame901.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame933.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame5.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3672.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame5.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame0.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame5.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3675.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame927.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame8.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame935.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame0.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame9.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame10.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3676.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame1.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame3.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame9.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame904.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame932.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame8.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame7.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame7.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame10.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame933.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame927.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame6.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame2.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame898.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame8.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame905.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame2.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame4.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame931.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame934.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame903.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame911.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame7.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame926.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame6.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame1.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame4.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame6.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame924.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame9.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3674.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame3673.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame907.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame10.jpg  \n",
            "  inflating: LPCV/train/images/7p3b_frame2.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame926.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame932.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame929.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame0.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame2.jpg  \n",
            "  inflating: LPCV/train/images/5p5b_frame925.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame909.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame934.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame0.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame3.jpg  \n",
            "  inflating: LPCV/train/images/4p1b_frame915.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame9.jpg  \n",
            "  inflating: LPCV/train/images/5p2b_frame3.jpg  \n",
            "  inflating: LPCV/train/images/5p4b_frame931.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD_SKy7ebSZC"
      },
      "source": [
        "%mkdir ./LPCV/valid\n",
        "%mkdir ./LPCV/valid/images\n",
        "%mkdir ./LPCV/valid/labels"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25xA556wy8uZ"
      },
      "source": [
        "def contrast_image(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.random_contrast(img, 2, 5, 42)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    copy_label(image, \"contrast\", folder)\n",
        "def noise_image(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.stateless_random_jpeg_quality(img, 2, 20, (42,42))\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    copy_label(image, \"noise\", folder)\n",
        "def grayscale_image(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.rgb_to_grayscale(img)\n",
        "    img = img.numpy()\n",
        "    img = img.reshape((img.shape[0], img.shape[1]))\n",
        "    im = Image.fromarray(img, 'L')\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    copy_label(image, \"grayscale\", folder)\n",
        "def saturate_image(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.random_saturation(img, 2, 120, 42)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    copy_label(image, \"saturate\", folder)\n",
        "def hue_image(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.random_hue(img, 0.1)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    copy_label(image, \"hue\", folder)\n",
        "def flip_image_lr(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.flip_left_right(img)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    flip_label_lr(image[:image.find('.')]+\".txt\", image[:image.find('.')]+\"_flip_lr.txt\", folder)\n",
        "def flip_image_ud(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.flip_up_down(img)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    flip_label_ud(image[:image.find('.')]+\".txt\", image[:image.find('.')]+\"_flip_ud.txt\", folder)\n",
        "def rot_90(image, new_image, folder):\n",
        "    img = Image.open(folder+'/images/'+image)\n",
        "    img = np.array(img)\n",
        "    img = tf.image.rot90(img)\n",
        "    img = img.numpy()\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(folder+'/images/'+new_image)\n",
        "    label_rot_90(image[:image.find('.')]+\".txt\", image[:image.find('.')]+\"_rot_90.txt\", folder)\n",
        "def flip_label_lr(label_file, new_label_file, folder):\n",
        "    label_file = folder+\"/labels/\"+label_file\n",
        "    new_label_file = folder+\"/labels/\"+new_label_file\n",
        "    with open(label_file) as f:\n",
        "        lines = f.readlines()\n",
        "        new_lines = []\n",
        "        for x in lines:\n",
        "            words = x.split(\" \")\n",
        "            words[1] = str(1-float(words[1]))\n",
        "            new_lines.append(\" \".join(words))\n",
        "    f.close()\n",
        "    with open(new_label_file, 'w') as f:\n",
        "        for x in new_lines:\n",
        "            f.write(x)\n",
        "def flip_label_ud(label_file, new_label_file, folder):\n",
        "    label_file = folder+\"/labels/\"+label_file\n",
        "    new_label_file = folder+\"/labels/\"+new_label_file\n",
        "    with open(label_file) as f:\n",
        "        lines = f.readlines()\n",
        "        new_lines = []\n",
        "        for x in lines:\n",
        "            words = x.split(\" \")\n",
        "            words[2] = str(1-float(words[2]))\n",
        "            new_lines.append(\" \".join(words))\n",
        "    f.close()\n",
        "    with open(new_label_file, 'w') as f:\n",
        "        for x in new_lines:\n",
        "            f.write(x)\n",
        "def label_rot_90(label_file, new_label_file, folder):\n",
        "    label_file = folder+\"/labels/\"+label_file\n",
        "    new_label_file = folder+\"/labels/\"+new_label_file\n",
        "    with open(label_file) as f:\n",
        "        lines = f.readlines()\n",
        "        new_lines = []\n",
        "        for x in lines:\n",
        "            words = x.split(\" \")\n",
        "            x = words[1]\n",
        "            y = words[2]\n",
        "            w = words[3]\n",
        "            h = words[4][:len(words[4])-1]\n",
        "            words[1] = y\n",
        "            words[2] = str(1-float(x))\n",
        "            words[3] = h\n",
        "            words[4] = w\n",
        "            new_lines.append(\" \".join(words))\n",
        "    f.close()\n",
        "    with open(new_label_file, 'w') as f:\n",
        "        for x in new_lines:\n",
        "            f.write(x)\n",
        "def copy_label(image, augmentation, folder):\n",
        "    filename = image[:image.find('.')]\n",
        "    new_filename = folder+'/labels/'+filename+'_'+augmentation+'.txt'\n",
        "    filename = folder+'/labels/'+filename+'.txt'\n",
        "    copyfile(filename, new_filename)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAd0zgmGzL6B"
      },
      "source": [
        "aug_list = ['_contrast', '_noise', '_flip_lr', '_flip_ud', '_rot_90', '_saturate', '_hue']\n",
        "data_dir = \"./LPCV/train\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x_exOYUzPrL"
      },
      "source": [
        "for root, dirs, files in os.walk(data_dir+'/images/', topdown=False):\n",
        "    for file in files:\n",
        "      count = 0\n",
        "      for j in aug_list:\n",
        "        filename = file[:file.find('.')]+j+file[file.find('.'):]\n",
        "        if count==0: contrast_image(file, filename, data_dir)\n",
        "        if count==1: noise_image(file, filename, data_dir)\n",
        "        if count==2: flip_image_lr(file, filename, data_dir)\n",
        "        if count==3: flip_image_ud(file, filename, data_dir)\n",
        "        if count==4: rot_90(file, filename, data_dir)\n",
        "        if count==5: saturate_image(file, filename, data_dir)\n",
        "        if count==6: hue_image(file, filename, data_dir)\n",
        "        count+=1"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxxm60lZXigF"
      },
      "source": [
        "for root, dirs, files in os.walk(data_dir+'/images/', topdown=False):\n",
        "    for file in files:\n",
        "        if file == '.DS_Store':\n",
        "            continue\n",
        "        filename = file[:file.find('.')]+\"_grayscale\"+file[file.find('.'):]\n",
        "        grayscale_image(file, filename, data_dir)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8qCdzcr5dK-"
      },
      "source": [
        "def moveFiles(src, dst):\n",
        "  files = []\n",
        "  for i in os.listdir(src):\n",
        "    files.append(i)\n",
        "  files = sorted(files)\n",
        "  listt = np.random.RandomState(seed=12).permutation(files)[:30]\n",
        "  for f in listt:\n",
        "    shutil.copy(os.path.join(src, f), dst)\n",
        "    p = os.path.join(src, f)\n",
        "    os.remove(p)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib6rOkLo5iZW"
      },
      "source": [
        "moveFiles(\"LPCV/train/images\", \"LPCV/valid/images\")\n",
        "moveFiles(\"LPCV/train/labels\", \"LPCV/valid/labels\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RKp4YPe53E0"
      },
      "source": [
        "data = open(\"data.yaml\", \"w\")\n",
        "data.write(\"train: ./LPCV/train/images\\nval: ./LPCV/valid/images\\nnc: 2\\nnames: [\\'person\\', \\'sports-ball\\']\")\n",
        "data.close()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htfcx4trJq4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18b4477-e26f-4d9d-ea3d-72abe1562068"
      },
      "source": [
        "!python train.py --img 416 --batch 150 --epochs 20 --data ./data.yaml --weights yolov5s.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=./data.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=20, batch_size=150, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v6.0-86-g7473f0f torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7025023 parameters, 7025023 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.001171875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'LPCV/train/labels' images and labels...1730 found, 0 missing, 0 empty, 0 corrupted: 100% 1730/1730 [00:00<00:00, 5281.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: LPCV/train/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'LPCV/valid/labels' images and labels...30 found, 0 missing, 0 empty, 0 corrupted: 100% 30/30 [00:00<00:00, 691.26it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: LPCV/valid/labels.cache\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.23, Best Possible Recall (BPR) = 0.9879\n",
            "Image sizes 416 train, 416 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     13.9G    0.1238   0.04281   0.02663      1217       416: 100% 12/12 [03:30<00:00, 17.57s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.53it/s]\n",
            "                 all         30        245     0.0671     0.0484     0.0138    0.00232\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     13.9G    0.1051   0.05157   0.01472      1254       416: 100% 12/12 [03:28<00:00, 17.37s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.64it/s]\n",
            "                 all         30        245      0.165      0.194       0.13     0.0265\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     13.9G   0.09171   0.04573  0.009901      1351       416: 100% 12/12 [03:26<00:00, 17.17s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.07it/s]\n",
            "                 all         30        245      0.571      0.319     0.0651     0.0129\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     13.9G   0.08951   0.03779  0.008356      1213       416: 100% 12/12 [03:18<00:00, 16.55s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.17it/s]\n",
            "                 all         30        245      0.818      0.209      0.268      0.107\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     13.9G   0.08389   0.03632  0.007571      1309       416: 100% 12/12 [03:21<00:00, 16.76s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.43it/s]\n",
            "                 all         30        245      0.725      0.242      0.225     0.0543\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     13.9G    0.0845   0.03424  0.006992      2165       416:  33% 4/12 [01:08<01:18,  9.77s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X2PXMAefeSY"
      },
      "source": [
        " !python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.05 --source 5p5b_03A1.m4v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SiWLouRZDIB",
        "outputId": "36976000-962c-4174-fe5b-598f00d231d5"
      },
      "source": [
        "!python val.py --weights runs/train/exp4/weights/best.pt --img 640 --data data.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=data.yaml, weights=['runs/train/exp4/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 ðŸš€ v6.0-86-g7473f0f torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'LPCV/valid/labels.cache' images and labels... 30 found, 0 missing, 0 empty, 0 corrupted: 100% 30/30 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:06<00:00,  6.97s/it]\n",
            "                 all         30        241      0.974      0.883      0.928      0.589\n",
            "              person         30        147      0.989          1      0.995      0.734\n",
            "         sports-ball         30         94       0.96      0.766      0.861      0.443\n",
            "Speed: 0.1ms pre-process, 3.5ms inference, 1.9ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PO482v6Olox",
        "outputId": "7576a8dd-b458-4b55-ba0a-3181b0b00ced"
      },
      "source": [
        "print(\"hi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ]
    }
  ]
}